{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93d6ed8d",
      "metadata": {
        "id": "93d6ed8d"
      },
      "source": [
        "# PREDICTION OF ANXIETY,DEPRESSION AND STRESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cef441e0",
      "metadata": {
        "id": "cef441e0",
        "outputId": "65b5fc4e-8b71-496b-ff80-1bbef192a24c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycountry_convert\n",
            "  Downloading pycountry_convert-0.7.2-py3-none-any.whl (13 kB)\n",
            "Collecting pytest-cov>=2.5.1\n",
            "  Downloading pytest_cov-4.0.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wheel>=0.30.0 in /usr/local/lib/python3.7/dist-packages (from pycountry_convert) (0.38.3)\n",
            "Collecting pycountry>=16.11.27.1\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 5.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytest>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pycountry_convert) (3.6.4)\n",
            "Collecting pytest-mock>=1.6.3\n",
            "  Downloading pytest_mock-3.10.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting pprintpp>=0.3.0\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting repoze.lru>=0.7\n",
            "  Downloading repoze.lru-0.7-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry>=16.11.27.1->pycountry_convert) (57.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (9.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (22.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (1.11.0)\n",
            "Collecting pytest>=3.4.0\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting coverage[toml]>=5.2.1\n",
            "  Downloading coverage-6.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 65.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.7/dist-packages (from coverage[toml]>=5.2.1->pytest-cov>=2.5.1->pycountry_convert) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (4.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (21.3)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
            "Collecting pytest>=3.4.0\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 2.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 35.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 52.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 3.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 36.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.0.0-py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 58.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest>=3.4.0->pycountry_convert) (0.10.2)\n",
            "  Downloading pytest-6.2.4-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 59.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 74.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.2-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 69.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 51.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.0-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 56.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 39.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.1-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 65.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.0-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 4.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.2-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 4.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 22.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.0-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 28.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.3-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 20.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.2-py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 38.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.1-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 12.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.0-py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 33.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.5-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 60.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 61.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.3-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 57.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.2-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 59.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.1-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 58.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.0-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 42.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.4-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 71.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.3-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 47.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.2-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 14.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.1-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 65.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.0-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 62.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 56.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.2-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 53.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.1-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 73.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.0-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 59.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.0.1-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 25.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.0.0-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 2.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.11-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 2.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.10-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 56.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.9-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.8-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 58.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.7-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 57.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.6-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 51.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.5-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 57.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.4-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 74.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.3-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 69.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.2-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 74.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.1-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 46.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 61.9 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of coverage to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of coverage[toml] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting coverage[toml]>=5.2.1\n",
            "  Downloading coverage-6.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 59.0 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.4.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 46.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.4.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 56.6 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.4.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 50.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 60.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 62.2 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 53.1 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of coverage to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of coverage[toml] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading coverage-6.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 29.6 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 63.5 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 64.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 57.0 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 56.7 MB/s \n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading coverage-6.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.0.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 57.0 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (252 kB)\n",
            "\u001b[K     |████████████████████████████████| 252 kB 56.8 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (252 kB)\n",
            "\u001b[K     |████████████████████████████████| 252 kB 20.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 22.4 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.4-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 23.1 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.3.1-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 45.7 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.3-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 42.8 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.2.1-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 56.3 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-cov to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-cov>=2.5.1\n",
            "  Downloading pytest_cov-3.0.0-py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.12.1-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.12.0-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.11.0-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.10.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting coverage>=4.4\n",
            "  Downloading coverage-5.2-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 59.5 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.1-cp37-cp37m-manylinux1_x86_64.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.4-cp37-cp37m-manylinux1_x86_64.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 57.7 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.3-cp37-cp37m-manylinux1_x86_64.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 54.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.2-cp37-cp37m-manylinux1_x86_64.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 74.1 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.1-cp37-cp37m-manylinux1_x86_64.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 65.0 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0-cp37-cp37m-manylinux1_x86_64.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 60.5 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.4-cp37-cp37m-manylinux1_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 57.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.3-cp37-cp37m-manylinux1_x86_64.whl (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 55.0 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.2-cp37-cp37m-manylinux1_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.1-cp37-cp37m-manylinux1_x86_64.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 68.4 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.tar.gz (378 kB)\n",
            "\u001b[K     |████████████████████████████████| 378 kB 70.8 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.4.2.tar.gz (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 63.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.4.1.tar.gz (369 kB)\n",
            "\u001b[K     |████████████████████████████████| 369 kB 51.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.4.tar.gz (369 kB)\n",
            "\u001b[K     |████████████████████████████████| 369 kB 14.4 MB/s \n",
            "\u001b[?25hCollecting pytest-cov>=2.5.1\n",
            "  Downloading pytest_cov-2.10.0-py2.py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of pytest-cov to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading pytest_cov-2.9.0-py2.py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of pytest-mock to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-mock>=1.6.3\n",
            "  Downloading pytest_mock-3.9.0-py3-none-any.whl (9.1 kB)\n",
            "  Downloading pytest_mock-3.8.2-py3-none-any.whl (9.1 kB)\n",
            "  Downloading pytest_mock-3.8.1-py3-none-any.whl (9.1 kB)\n",
            "  Downloading pytest_mock-3.8.0-py3-none-any.whl (9.1 kB)\n",
            "  Downloading pytest_mock-3.7.0-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.6.1-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.6.0-py3-none-any.whl (12 kB)\n",
            "INFO: pip is looking at multiple versions of pytest-mock to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading pytest_mock-3.5.1-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.5.0-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.4.0-py3-none-any.whl (11 kB)\n",
            "  Downloading pytest_mock-3.3.1-py3-none-any.whl (11 kB)\n",
            "  Downloading pytest_mock-3.3.0-py3-none-any.whl (11 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading pytest_mock-3.2.0-py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: pycountry\n",
            "  Building wheel for pycountry (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=3de75ebaf79508912336a4398891c4b097a90f0d3f36991b5b3b0cb51900c71a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/06/e8/7ee176e95ea9a8a8c3b3afcb1869f20adbd42413d4611c6eb4\n",
            "Successfully built pycountry\n",
            "Installing collected packages: coverage, repoze.lru, pytest-mock, pytest-cov, pycountry, pprintpp, pycountry-convert\n",
            "Successfully installed coverage-6.5.0 pprintpp-0.4.0 pycountry-22.3.5 pycountry-convert-0.7.2 pytest-cov-2.9.0 pytest-mock-3.2.0 repoze.lru-0.7\n"
          ]
        }
      ],
      "source": [
        "# basic libraries - musts\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#impute missings with KNN\n",
        "from sklearn.impute import KNNImputer\n",
        "#to convert country codes to alpha 3\n",
        "!pip install pycountry_convert\n",
        "from pycountry_convert import country_alpha2_to_country_name, country_name_to_country_alpha3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "W4Q4e_eIfQro"
      },
      "id": "W4Q4e_eIfQro",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFa3yE06SyxX"
      },
      "id": "JFa3yE06SyxX",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlIhzi6uS20r",
        "outputId": "04560ae3-faa1-4bd5-876d-75574f219d5d"
      },
      "id": "VlIhzi6uS20r",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tEnTKifS9gQ",
        "outputId": "81dde211-e42f-4edb-dbb5-6126812b4033"
      },
      "id": "2tEnTKifS9gQ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20384be4",
      "metadata": {
        "id": "20384be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "ab07d6e5-4c12-45dd-e581-484fd542a007"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a20585388d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/drive/MyDrive/data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/drive/MyDrive/data.csv'"
          ]
        }
      ],
      "source": [
        "dataframe = pd.read_csv('/drive/MyDrive/data.csv', delimiter='\\t')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f05c7d",
      "metadata": {
        "id": "c4f05c7d"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a657ee23",
      "metadata": {
        "id": "a657ee23"
      },
      "outputs": [],
      "source": [
        "dataframe.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521e6e69",
      "metadata": {
        "id": "521e6e69"
      },
      "source": [
        "The DASS is a 42-item self-administered questionnaire designed to measure the magnitude of three negative emotional states: depression, anxiety, and stress. The DASSDepression focuses on reports of low mood, motivation, and self-esteem, DASS-anxiety on physiological arousal, perceived panic, and fear, and DASS-stress on tension and irritability. Instructions to client and scoring: A respondent indicates on a 4-point scale the extent to which each of 42 statements applied over the past week. A printed overlay is used to obtain total scores for each subscale. Higher scores on each subscale indicate increasing severity of depression, anxiety, or stress\n",
        "\n",
        "The DASS has 3 parts;\n",
        "\n",
        "    0 - 14 : Depression\n",
        "    15 - 28 : Anxiety\n",
        "    29 - 42 : Stress\n",
        "\n",
        "And these three parts were evaluated in 3 parts also;\n",
        "\n",
        "    Physical Symptoms\n",
        "    Mental Symptoms\n",
        "    Total Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2249e0b4",
      "metadata": {
        "id": "2249e0b4"
      },
      "outputs": [],
      "source": [
        "#Calculating DASS scores\n",
        "df = dataframe.copy() # taking a copy for bakup\n",
        "df.iloc[:,0:126]\n",
        "#A - Question 1 to 42 are stored in A(eg:Q1A),timetaken in milliseconds to answer the qn is stored in E(Q1E) \n",
        "#and the question's position in the survey is stored in I(Q1I) \n",
        "#The questionnaired has three subscales(D,A,S) with 14 items in each subscale for the DASS 42 and the max score of 42 item is 126\n",
        "\n",
        "questions = [i for i in df.iloc[:,0:126] if  'A' in i]\n",
        "time = [i for i in df.iloc[:,0:126] if  'E' in i] # should be equal to testelapse\n",
        "position = [i for i in df.iloc[:,0:126] if  'I' in i]\n",
        "\n",
        "# save items in another dataframe\n",
        "item_positions = df[position] \n",
        "dass = df[questions]\n",
        "testelapse = df[time]\n",
        "\n",
        "df.drop(position, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1d37c1",
      "metadata": {
        "id": "4d1d37c1"
      },
      "outputs": [],
      "source": [
        "# DASS score calculation\n",
        "## Q2, Q4, Q7, Q15, Q19, Q23, Q25, Q41 questions are categorized as physical symptoms\n",
        "# other questions are categorized as mental symptoms\n",
        "##  _tot = totalqns, _physical_sym_qns = physical symptom, _mental_sym_qns = mental_symptom\n",
        "\n",
        "df['depression_totalqns'] = np.sum(dass.iloc[:,0:14],axis=1)\n",
        "df['depression_physical_sym_qns'] = np.sum(dass.loc[:,['Q2A','Q4A','Q7A']],axis=1)\n",
        "df['depression_mental_sym_qns'] = df['depression_totalqns'] - df['depression_physical_sym_qns']\n",
        "df['anxiety_totalqns'] = np.sum(dass.iloc[:,15:28],axis=1)\n",
        "df['anxiety_physical_sym_qns'] = np.sum(dass.loc[:,['Q15A','Q19A','Q23A','Q25A']],axis=1)\n",
        "df['anxiety_mental_sym_qns'] = df['anxiety_totalqns'] - df['anxiety_physical_sym_qns']\n",
        "df['stress_totalqns'] = np.sum(dass.iloc[:,29:42],axis=1)\n",
        "df['stress_physical_sym_qns'] = np.sum(dass.loc[:,['Q41A']],axis=1)\n",
        "df['stress_mental_sym_qns'] = df['stress_totalqns'] - df['stress_physical_sym_qns']\n",
        "df['total'] = np.sum(dass.iloc[:,0:42], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef77245",
      "metadata": {
        "id": "9ef77245"
      },
      "outputs": [],
      "source": [
        "#total time is not equal to testelapse.\n",
        "df['testelapse'] = np.sum(df[time],axis=1) / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e53ea7d",
      "metadata": {
        "id": "1e53ea7d"
      },
      "outputs": [],
      "source": [
        "# now drop time related features\n",
        "df.drop(df[time],axis=1, inplace=True)\n",
        "df.drop(df[questions],axis=1, inplace= True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1274d41a",
      "metadata": {
        "id": "1274d41a"
      },
      "source": [
        "# Ten item Personality Inventory(TIPI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef191383",
      "metadata": {
        "id": "ef191383"
      },
      "source": [
        "The Ten-Item Personality Inventory (TIPI) is a brief assessment of the Big Five personality dimensions:\n",
        "(1) Extraversion, (2) Agreeableness, (3) Conscientiousness, (4) Emotional Stability, and (5) Openness to Experience. \n",
        "Items are rated on a scale from 1, disagree strongly, to 7, agree strongly.\n",
        "\n",
        "TIPI Scoring is done on the basis of this article:https://gosling.psy.utexas.edu/scales-weve-developed/ten-item-personality-measure-tipi/\n",
        "\n",
        "Scoring the TIPI\n",
        "\n",
        "1. Recode the reverse-scored items (i.e., recode a 7 with a 1, a 6 with a 2, a 5 with a 3, etc.). The reverse scored items are 2, 4, 6, 8, & 10.\n",
        "\n",
        "2. Take the AVERAGE of the two items (the standard item and the recoded reverse-scored item) that make up each scale.\n",
        "\n",
        "Example using the Extraversion scale: A participant has scores of 5 on item 1 (Extraverted, enthusiastic) and and 2 on item 6 (Reserved, quiet). First, recode the reverse-scored item (i.e., item 6), replacing the 2 with a 6. Second, take the average of the score for item 1 and the (recoded) score for item 6. So the TIPI Extraversion scale score would be: (5 + 6)/2 = 5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97fa85cf",
      "metadata": {
        "id": "97fa85cf"
      },
      "outputs": [],
      "source": [
        "# TIPI\n",
        "conversion = {1: 7,\n",
        "              2: 6,\n",
        "              3: 5,\n",
        "              4: 4,\n",
        "              5: 3,\n",
        "              6: 2,\n",
        "              7: 1}\n",
        "\n",
        "#negative items are 2 ,4, 6, 8, 10\n",
        "negative_items = ['TIPI2', 'TIPI4', 'TIPI6', 'TIPI8', 'TIPI10']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aa9493c",
      "metadata": {
        "id": "5aa9493c"
      },
      "outputs": [],
      "source": [
        "print(conversion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b27ec0ba",
      "metadata": {
        "id": "b27ec0ba"
      },
      "outputs": [],
      "source": [
        "for x in negative_items:\n",
        "    df[x] = df[x].map(conversion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7ba902",
      "metadata": {
        "id": "bc7ba902"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f903ad9",
      "metadata": {
        "id": "3f903ad9"
      },
      "outputs": [],
      "source": [
        "tipi = df[[i for i in df.columns if 'TIPI' in i]]\n",
        "\n",
        "df['TIPI_extraversion'] = (df['TIPI1'] + df['TIPI6']) / 2\n",
        "df['TIPI_agreeableness'] = (df['TIPI2'] + df['TIPI7']) / 2\n",
        "df['TIPI_conscientiousness'] = (df['TIPI3'] + df['TIPI8']) / 2\n",
        "df['TIPI_emotional_stability'] = (df['TIPI4'] + df['TIPI9']) / 2\n",
        "df['TIPI_openness_exp'] = (df['TIPI5'] + df['TIPI10']) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddffde43",
      "metadata": {
        "id": "ddffde43"
      },
      "outputs": [],
      "source": [
        "# drop tipi\n",
        "df.drop(tipi, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3cc52c",
      "metadata": {
        "id": "3b3cc52c"
      },
      "source": [
        "VCL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccea16cb",
      "metadata": {
        "id": "ccea16cb"
      },
      "source": [
        "A value of 1 is checked, 0 means unchecked. The words at VCL6, VCL9, and VCL12 are not real words and can be used as a validity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3087506e",
      "metadata": {
        "id": "3087506e"
      },
      "outputs": [],
      "source": [
        "VCL_questions = df[[i for i in df.columns if 'VCL' in i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2bdbb03",
      "metadata": {
        "id": "c2bdbb03"
      },
      "outputs": [],
      "source": [
        "# renaming VCL's with words\n",
        "names = ['boat',\n",
        "         'incoherent', \n",
        "         'pallid',\n",
        "         'robot',\n",
        "         'audible', \n",
        "         'reliability_q1', \n",
        "         'paucity',\n",
        "         'epistemology',\n",
        "         'reliability_q2',\n",
        "         'decide',\n",
        "         'pastiche',\n",
        "         'reliability_q3',\n",
        "         'abysmal',\n",
        "         'lucid',\n",
        "         'betray',\n",
        "         'funny']\n",
        "\n",
        "for x,y in enumerate(names):\n",
        "    df.rename(columns = {'VCL'+str(x+1) : y}, inplace = True)\n",
        "\n",
        "\n",
        "for x in names:\n",
        "    df[x] = df[x].astype('object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b90bfec",
      "metadata": {
        "id": "3b90bfec"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5424f13a",
      "metadata": {
        "id": "5424f13a"
      },
      "outputs": [],
      "source": [
        "# change categories to category / object type\n",
        "categories = ['source', 'uniquenetworklocation', 'screensize', 'country',\n",
        "'major', 'familysize', 'married', 'voted', 'race',  'orientation', 'religion',\n",
        "'religion', 'hand', 'gender','engnat', 'urban', 'education']\n",
        "\n",
        "for column in categories:\n",
        "    df[column] = df[column].astype('object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd587eca",
      "metadata": {
        "id": "cd587eca"
      },
      "outputs": [],
      "source": [
        "print(df[column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0080243c",
      "metadata": {
        "id": "0080243c"
      },
      "outputs": [],
      "source": [
        "#Simplifying Major column to cluster everything similar and to replace nAn with OTHER\n",
        "\n",
        "df['major'] = df['major'].str.lower()\n",
        "df['major'].value_counts().nlargest(n=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529fea48",
      "metadata": {
        "id": "529fea48"
      },
      "outputs": [],
      "source": [
        "#function for cleaning 'major' column\n",
        "#'na' values changed with 'other' value\n",
        "\n",
        "def major_simplify(title):\n",
        "    if 'business management' in str(title).lower():\n",
        "        return 'management'\n",
        "    elif 'information technology' in str(title).lower():\n",
        "        return 'it'\n",
        "    elif 'math' in str(title).lower():\n",
        "        return 'mathematics'\n",
        "    elif 'computer' in str(title).lower():\n",
        "        return 'it'\n",
        "    elif 'bio' in str(title).lower():\n",
        "        return 'biology'\n",
        "    elif 'tesl' in str(title).lower():\n",
        "        return 'english'\n",
        "    elif 'medic' in str(title).lower():\n",
        "        return 'medicine'\n",
        "    elif 'account' in str(title).lower():\n",
        "        return 'accountacy'\n",
        "    elif 'none' in str(title).lower():\n",
        "        return np.nan\n",
        "    elif 'nurs' in str(title).lower():\n",
        "        return 'nursing'\n",
        "    elif '-' in str(title).lower():\n",
        "        return np.nan\n",
        "    elif 'teach' in str(title).lower():\n",
        "        return 'teaching'\n",
        "    elif 'pharma' in str(title).lower():\n",
        "        return 'pharmacy'\n",
        "    elif 'no' in str(title).lower():\n",
        "        return np.nan\n",
        "    elif 'film' in str(title).lower():\n",
        "        return 'media'\n",
        "    elif 'international' in str(title).lower():\n",
        "        return 'international relations'\n",
        "    elif 'human' in str(title).lower():\n",
        "        return 'human resources'\n",
        "    elif 'art' in str(title).lower():\n",
        "        return 'arts'\n",
        "    elif 'islam' in str(title).lower():\n",
        "        return 'islamic studies'\n",
        "    elif 'physio' in str(title).lower():\n",
        "        return 'physiotherapy'\n",
        "    elif 'socio' in str(title).lower() or 'social' in str(title).lower():\n",
        "        return 'sociology'\n",
        "    elif 'bank' in str(title).lower():\n",
        "        return 'banking'\n",
        "    elif 'agri' in str(title).lower():\n",
        "        return 'agriculture'\n",
        "    elif 'commerce' in str(title).lower() or 'real estate' in str(title).lower():\n",
        "        return 'marketing'\n",
        "    elif 'counsel' in str(title).lower():\n",
        "        return 'counselling'\n",
        "    elif 'programming' in str(title).lower():\n",
        "        return 'it'\n",
        "    elif 'civil' in str(title).lower():\n",
        "        return 'engineering'\n",
        "    elif 'ict' in str(title).lower():\n",
        "        return 'it'\n",
        "    elif 'communication' in str(title).lower():\n",
        "        return 'communication'\n",
        "    elif 'administration' in str(title).lower():\n",
        "        return 'administration'\n",
        "    elif 'psycho' in str(title).lower():\n",
        "        return 'psychology'\n",
        "    elif 'english' in str(title).lower():\n",
        "        return 'english'\n",
        "    elif 'law' in str(title).lower():\n",
        "        return 'laws'\n",
        "    elif 'engineering' in str(title).lower():\n",
        "        return 'engineering'\n",
        "    elif 'architecture' in str(title).lower():\n",
        "        return 'architecture'\n",
        "    elif 'design' in str(title).lower():\n",
        "        return 'designer'\n",
        "    else:\n",
        "        return title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc644cde",
      "metadata": {
        "id": "fc644cde"
      },
      "outputs": [],
      "source": [
        "#call the simplify function\n",
        "\n",
        "df['major'] = df['major'].apply(major_simplify)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c3e25c",
      "metadata": {
        "id": "a1c3e25c"
      },
      "outputs": [],
      "source": [
        "#Fixing Education and Major column\n",
        "#The education column has some errors. It should have 1,2,3 and 4; however it has 0's\n",
        "#Major column has some 0,1 and 2 values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f533a2",
      "metadata": {
        "id": "b0f533a2"
      },
      "outputs": [],
      "source": [
        "df['education'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbf82d0",
      "metadata": {
        "id": "8dbf82d0"
      },
      "outputs": [],
      "source": [
        "df[df['education']==0]['major'].notnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8cf17fa",
      "metadata": {
        "id": "b8cf17fa"
      },
      "outputs": [],
      "source": [
        "#Change Education '0' to value '3' where major is not null\n",
        "eduzero = df[(df['major'].notnull()) & ((df['education'] == 0))]\n",
        "eduzero['education'] = 3\n",
        "df.loc[eduzero.index, 'education'] = eduzero['education']\n",
        "\n",
        "#changed major '0' to 'no degree' where Education is equal to 0.\n",
        "#change 0's major to 'no degree', and 0 to 1\n",
        "majorzero = df[(df['major'].isnull()) & ((df['education'] == 0))]\n",
        "majorzero['major'] = 'without a degree'\n",
        "majorzero['education'] = 1\n",
        "df.loc[majorzero.index, 'major'] = majorzero['major']\n",
        "df.loc[majorzero.index, 'education'] = majorzero['education']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94328d5b",
      "metadata": {
        "id": "94328d5b"
      },
      "outputs": [],
      "source": [
        "#Change education with value 1 to 3 where major is is not null and education is equal to 1\n",
        "# change 1's to 3 \n",
        "eduone = df[(df['major'].notnull()) & ((df['education'] == 1))]\n",
        "eduone['education'] = 3\n",
        "df.loc[eduone.index, 'education'] = eduone['education']\n",
        "\n",
        "#Change major with value 1 to 'without a degree' where education is equal to 1\n",
        "#change 1's major to 'without a degree'\n",
        "majorone = df[(df['major'].isnull()) & ((df['education'] == 1))]\n",
        "majorone['major'] = 'without a degree'\n",
        "df.loc[majorone.index, 'major'] = majorone['major']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3231bdf",
      "metadata": {
        "id": "d3231bdf"
      },
      "outputs": [],
      "source": [
        "#Change education with value 2 to 3 where major is is not null and education is equal to 2\n",
        "# change 2's to 3 \n",
        "edutwo = df[(df['major'].notnull()) & ((df['education'] == 2))]\n",
        "edutwo['education'] = 3\n",
        "df.loc[edutwo.index, 'education'] = edutwo['education']\n",
        "\n",
        "#Change major with value 2 to 'without a degree' where education is equal to 2\n",
        "#change 2's major to 'no degree'\n",
        "majortwo = df[(df['major'].isnull()) & ((df['education'] == 2))]\n",
        "majortwo['major'] = 'without a degree'\n",
        "df.loc[majortwo.index, 'major'] = majortwo['major']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45f4d64",
      "metadata": {
        "id": "f45f4d64"
      },
      "outputs": [],
      "source": [
        "df['education'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc00ac9",
      "metadata": {
        "id": "bcc00ac9"
      },
      "outputs": [],
      "source": [
        "df['education'] = df['education'].astype('object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e09abd6",
      "metadata": {
        "id": "7e09abd6"
      },
      "outputs": [],
      "source": [
        "df['major'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec64e4f3",
      "metadata": {
        "id": "ec64e4f3"
      },
      "outputs": [],
      "source": [
        "#Major NA values filled with Mode\n",
        "df['major'] = df.groupby('education')['major'].apply(lambda x: x.fillna(x.mode()[0]))\n",
        "df['major'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a87bbb",
      "metadata": {
        "id": "91a87bbb"
      },
      "outputs": [],
      "source": [
        "## changing major names to 'other' if less than 60\n",
        "major = df['major'].value_counts()\n",
        "df['major'] = np.where(df['major'].isin(major.index[major < 60]), 'Other', df['major'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8bfc8e",
      "metadata": {
        "id": "dc8bfc8e"
      },
      "outputs": [],
      "source": [
        "#replace 'no' to 'without a degree' and 'without a degree' to 'no degree'\n",
        "df['major'] = df['major'].apply(lambda x: x.replace('no','without a degree'))\n",
        "df['major'] = df['major'].apply(lambda x: x.replace('without a degree','no degree'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6198adc",
      "metadata": {
        "id": "e6198adc"
      },
      "outputs": [],
      "source": [
        "df['major']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2196310",
      "metadata": {
        "id": "e2196310"
      },
      "outputs": [],
      "source": [
        "#Cleaning Age,it has some unrealistic values\n",
        "#Some inputs are based on birth year so we are changing those inputs to age\n",
        "\n",
        "df['age'] = df['age'].apply(lambda x: 2019-x if x > 1798 else x)\n",
        "\n",
        "#drop age > 100\n",
        "df.drop(df[df['age'] > 100].index, inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9360caec",
      "metadata": {
        "id": "9360caec"
      },
      "outputs": [],
      "source": [
        "#Some more data cleaning\n",
        "\n",
        "#replace value of gender 0 with 3\n",
        "df['gender'].replace({0:3},inplace=True)\n",
        "\n",
        "#replace value of orientation 0 with 5\n",
        "df['orientation'].replace({0:5}, inplace=True)\n",
        "\n",
        "#replace value of religion 0 with 2\n",
        "df['religion'].replace({0:2}, inplace=True)\n",
        "\n",
        "#replace value of married 0 with 3 - other\n",
        "df['married'].replace({0:3}, inplace=True)\n",
        "\n",
        "#replace value of hand 0 with 3\n",
        "df['hand'].replace({0:3}, inplace = True)\n",
        "\n",
        "#Changing zeros to four as a other\n",
        "df['urban'].replace({0:4}, inplace=True)\n",
        "\n",
        "df.drop(df[df['familysize'] > 14].index, inplace=True)\n",
        "\n",
        "#replace value of familysize 0 with 1\n",
        "df['familysize'].replace({0:1}, inplace=True)\n",
        "\n",
        "#replace value of voted 0 with 2\n",
        "df['voted'].replace({0:2}, inplace=True)\n",
        "\n",
        "# Impute missings in TIPI\n",
        "imputer = KNNImputer()\n",
        "\n",
        "##KNNimputer for missing values\n",
        "df.loc[:,['TIPI_agreeableness','TIPI_extraversion',\n",
        "          'TIPI_conscientiousness','TIPI_openness_exp','TIPI_emotional_stability']] = imputer.fit_transform(df.loc[:,['TIPI_agreeableness','TIPI_extraversion',\n",
        "          'TIPI_conscientiousness','TIPI_openness_exp','TIPI_emotional_stability']]) \n",
        "\n",
        "##filling null values in country column with mode\n",
        "df['country'] = df['country'].fillna(df['country'].mode()[0])\n",
        "df['country'] = df['country'].apply(lambda x: x.replace('NONE','MY'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a27d3a",
      "metadata": {
        "id": "66a27d3a"
      },
      "outputs": [],
      "source": [
        "df['country']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50c58f6",
      "metadata": {
        "id": "d50c58f6"
      },
      "outputs": [],
      "source": [
        "## Changing 'XK' code to 'RS' code because 'XK' code not in 'country_alpha2_to_country_name'\n",
        "df['country'] = df['country'].apply(lambda x: x.replace('XK','RS'))\n",
        "\n",
        "##Converting ISO 2 code to ISO 3 code\n",
        "df['country_alpha_3'] = df['country'].apply(lambda x: country_name_to_country_alpha3(country_alpha2_to_country_name(x)))\n",
        "df.drop(['country'], axis = 1, inplace=True)\n",
        "\n",
        "df['country_alpha_3'].value_counts().nlargest(n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095c843a",
      "metadata": {
        "id": "095c843a"
      },
      "outputs": [],
      "source": [
        "df['country_alpha_3'].value_counts().nlargest(n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b07e5b7",
      "metadata": {
        "id": "7b07e5b7"
      },
      "outputs": [],
      "source": [
        "#There were 16 items in VCL, it was asked examinee's whether they know or don't know given vocabulary. \n",
        "#3 given words are not actual words which are used as reliability items.\n",
        "#Hence We created a sumation of VCL; each correct word is +1, and reliability items are -1 each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7658e5a2",
      "metadata": {
        "id": "7658e5a2"
      },
      "outputs": [],
      "source": [
        "VCL_questions.columns\n",
        "VCL_positives = ['VCL1', 'VCL2', 'VCL3', 'VCL4', 'VCL5','VCL7', 'VCL8', 'VCL10', 'VCL11', \n",
        "                 'VCL13', 'VCL14', 'VCL15', 'VCL16']\n",
        "VCL_negatives = ['VCL6', 'VCL9', 'VCL12']\n",
        "\n",
        "df['total_vocabulary'] = np.sum(VCL_questions[VCL_positives], axis=1) - np.sum(VCL_questions[VCL_negatives],axis=1)                                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94350068",
      "metadata": {
        "id": "94350068"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "1HD6BadByhxd"
      },
      "id": "1HD6BadByhxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['total']"
      ],
      "metadata": {
        "id": "tFvimOYnWSDk"
      },
      "id": "tFvimOYnWSDk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "df['major'].value_counts().sort_values(ascending= True).plot(kind='barh', \n",
        "                                                             width = 0.9,\n",
        "                                                             color=list('bgc'))\n",
        "\n",
        "for i, v in enumerate(df['major'].value_counts().sort_values(ascending= True)):\n",
        "    plt.text(v + 10, i - 0.45, str(v), color='blue', fontweight='bold')\n",
        "\n",
        "_ = plt.xlabel(\"Number of Examinees\")\n",
        "_ = plt.ylabel(\"Major Names\")\n",
        "_ = plt.title('Total Number of Examinees by Each Major')\n"
      ],
      "metadata": {
        "id": "3sg_U0ADetdW"
      },
      "id": "3sg_U0ADetdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "df['country_alpha_3'].value_counts()[:25].sort_values(ascending= True).plot(kind='barh', \n",
        "                                                             width = 0.9,\n",
        "                                                             color=list('bgc'))\n",
        "\n",
        "for i, v in enumerate(df['country_alpha_3'].value_counts()[:25].sort_values(ascending= True)):\n",
        "    plt.text(v + 10, i - 0.15, str(v), color='blue', fontweight='bold')\n",
        "\n",
        "_ = plt.xlabel(\"Number of Examinees\")\n",
        "_ = plt.ylabel(\"Countries\")\n",
        "_ = plt.title('Total Number of Examinees by Each Country')"
      ],
      "metadata": {
        "id": "PdlwO954fabd"
      },
      "id": "PdlwO954fabd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO PLOT STACKING BARH for DASS ITEMS\n",
        "\n",
        "# first create a DF with value_counts of each item\n",
        "counts = []\n",
        "for col in dass.columns:\n",
        "  counts.append([dass[col].value_counts()[1], \n",
        "                dass[col].value_counts()[2],\n",
        "                dass[col].value_counts()[3],\n",
        "                dass[col].value_counts()[4]])\n",
        "  \n",
        "#convert list to DF\n",
        "count = pd.DataFrame(counts, columns =['Did not apply to me at all',\n",
        "                                       'Applied to me some degree',\n",
        "                                       'Applied to me considerable degree',\n",
        "                                       'Applied to me very much'])\n",
        "\n",
        "count['item'] = dass.columns\n",
        "# extract numbers in Items column\n",
        "count['item'] = count.item.str.extract(('(\\d+)'))\n",
        "#save item as INT\n",
        "count['item'] = count['item'].astype('int32') \n",
        "\n"
      ],
      "metadata": {
        "id": "mb1D6yS4fpff"
      },
      "id": "mb1D6yS4fpff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now plot\n",
        "#create a plot object\n",
        "_ = plt.style.use('ggplot')\n",
        "\n",
        "_ = count.sort_values('item', ascending=False).plot(x='item', \n",
        "           kind='barh',\n",
        "           stacked=True,\n",
        "           width = 0.85, \n",
        "           legend = True,\n",
        "           colormap='Spectral',\n",
        "           figsize= (12,10)\n",
        "           )\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Questions\")\n",
        "\n",
        "plt.legend( bbox_to_anchor = (1.1, -0.1), ncol=len(count.columns))\n",
        "\n",
        "# add count of each response\n",
        "for p in _.patches:\n",
        "    left, bottom, width, height = p.get_bbox().bounds\n",
        "    if width > 0:\n",
        "         _.annotate(f'{width:0.0f}', xy=(left+width/2, bottom+height/2), ha='center', va='center',)"
      ],
      "metadata": {
        "id": "hC4G7A2-fvFQ"
      },
      "id": "hC4G7A2-fvFQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following items were answered as the examinee's felt that way most of the time:\n",
        "\n",
        "Question 11: I found myself getting upset rather easily.\n",
        "Question 13: I felt sad and depressed.\n",
        "Question 18: I felt that I was rather touchy.\n",
        "Question 34: I felt I was pretty worthless.\n",
        "Question 40: I was worried about situations in which I might panic and make a fool of myself.\n",
        "On the other hand, following items were answered as the examinee's felt that way nearly never:\n",
        "\n",
        "Question 15: I had a feeling of faintness.\n",
        "Question 23: I had difficulty in swallowing.\n",
        "It is obvious that most of the participants had mental / emotional symptoms of ADS; and do not have physical symptoms."
      ],
      "metadata": {
        "id": "1Nm7fmtEf_8Q"
      },
      "id": "1Nm7fmtEf_8Q"
    },
    {
      "cell_type": "code",
      "source": [
        "selected_questions = ['Q2E','Q4E','Q7E','Q15E','Q19E','Q23E','Q25E','Q41E']\n",
        "ps_questions = dict(testelapse[selected_questions])\n",
        "all_questions = dict(testelapse)\n",
        "\n",
        "colors = []\n",
        "for i in all_questions.keys():\n",
        "    if i in ps_questions.keys():\n",
        "      colors.append('g')\n",
        "    else:\n",
        "      colors.append('b')"
      ],
      "metadata": {
        "id": "wZMiHeJdf-tp"
      },
      "id": "wZMiHeJdf-tp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "testelapse[[x for x in testelapse.columns if 'E' in x]].mean().plot(kind='barh', \n",
        "                                                                    width = 0.9,\n",
        "                                                                    color=colors,\n",
        "                                                                  )\n",
        "_ = plt.xlabel(\"Spent Time\")\n",
        "_ = plt.ylabel(\"Questions\")\n",
        "_ = plt.title(\"Distribution of Spent Time per Question\")"
      ],
      "metadata": {
        "id": "-_XwauGDgIZn"
      },
      "id": "-_XwauGDgIZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest time spent on Question 9, followed by 35 and 25.\n",
        "\n",
        "Q9 I found myself in situations that made me so anxious I was most relieved when they ended.\n",
        "\n",
        "Q25 I was aware of the action of my heart in the absence of physical exertion (eg, sense of heart rate increase, heart missing a beat).\n",
        "\n",
        "Q35 I was intolerant of anything that kept me from getting on with what I was doing."
      ],
      "metadata": {
        "id": "fGbT-ksegXMH"
      },
      "id": "fGbT-ksegXMH"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "xY0wAaDBg3ua"
      },
      "id": "xY0wAaDBg3ua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores amongst gender, race, religion and orientation\n",
        "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(25,10))\n",
        "\n",
        "#ax1 : gender and total score means\n",
        "_ = sns.barplot( y = df['total'], x = df['gender'], ax= ax1)\n",
        "\n",
        "ax1.set_title('DASS Mean Scores by Gender')\n",
        "\n",
        "for i, v in enumerate(df['gender'].value_counts()):\n",
        "  ax1.text(s = f\"n : {v:}\", x = i - 0.1, y = 80 , fontsize= 12)\n",
        "ax1.set_xticklabels(['male','female','other'])   \n",
        "\n",
        "\n",
        "_ = sns.barplot( y = df['total'], x = df['race'], ax= ax2)\n",
        "\n",
        "for i, v in enumerate(df['race'].value_counts()):\n",
        "  ax2.text(s = f\"n : \\n{v:}\", x = i - 0.25, y = 80, fontsize=12)\n",
        "\n",
        "ax2.set_title('DASS Mean Scores by Race')\n",
        "\n",
        "#labels for race\n",
        "labels_race = ['Asian', \n",
        "               'Arab',\n",
        "               'Black', \n",
        "               'Indigenous', \n",
        "               'Native American', \n",
        "               'White', \n",
        "               'Other']\n",
        "_ = ax2.set_xticklabels(labels_race)\n",
        "\n",
        "_ = plt.figtext(0.5, 0.01,'n defines the total number of participants in each category',\n",
        "           ha='center', fontsize=12)"
      ],
      "metadata": {
        "id": "lgECsrKJgYQK"
      },
      "id": "lgECsrKJgYQK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minorities and disadventagenous groups have higher DASS scores. However, group participant distributions in this study are way too much unbalanced. Therefore, it is really hard to make general assumptions rather than having a general idea what is going on.\n",
        "\n",
        "Male gender obviously have lower DASS scores.\n",
        "\n",
        "It is also hard to make assumptions about race, however we can have a better idea of asian, arab and black participants. Arabs have the highest DASS score among three groups. Apart from groups; all race groups have DASS scores higher than 100."
      ],
      "metadata": {
        "id": "iW2qGZB1hCdp"
      },
      "id": "iW2qGZB1hCdp"
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
        "_ = sns.barplot( y = df['total'], x = df['religion'])\n",
        "\n",
        "religion_labels = ['Agnostic', \n",
        "                   'Atheist', \n",
        "                   'Buddhist',\n",
        "                   'Christian (Catholic)',\n",
        "                   'Christian (Mormon)',\n",
        "                   'Christian (Protestant)',\n",
        "                   'Christian (Other)',\n",
        "                   'Hindu', \n",
        "                   'Jewish', \n",
        "                   'Muslim', \n",
        "                   'Sikh',\n",
        "                   'Other',\n",
        "                   ]\n",
        "plt.title('Total DASS Scores by Religion')\n",
        "_ = ax.set_xticklabels(religion_labels, rotation= 60)\n",
        "\n",
        "\n",
        "for i, v in enumerate(df['religion'].value_counts()):\n",
        "  ax.text(s = f\"n : \\n{v:}\", x = i - 0.30, y = 80, fontsize=12)"
      ],
      "metadata": {
        "id": "6bvuULlzhD3z"
      },
      "id": "6bvuULlzhD3z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "_ = sns.barplot( y = df['total'], x = df['orientation'])\n",
        "\n",
        "orientation_labels = ['Heterosexual',\n",
        "                      'Bisexual',\n",
        "                      'Homosexual',\n",
        "                      'Asexual',\n",
        "                      'Other'\n",
        "                   ]\n",
        "plt.title('Total DASS Scores by Orientation')\n",
        "_ = ax.set_xticklabels(orientation_labels)\n",
        "\n",
        "\n",
        "for i, v in enumerate(df['orientation'].value_counts()):\n",
        "  ax.text(s = f\"n : {v:}\", x = i - 0.30, y = 80, fontsize=12)"
      ],
      "metadata": {
        "id": "KvKHoveOhTOD"
      },
      "id": "KvKHoveOhTOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bisexuals have the highest DASS score, and all others apart from Hetero's have higher scores than hetero's."
      ],
      "metadata": {
        "id": "mk2B4YfokLG6"
      },
      "id": "mk2B4YfokLG6"
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [13, 18, 30, 40, 50, 60, 70, 120]\n",
        "labels = ['13-17','18-29', '30-39', '40-49', '50-59', '60-69', '70+']\n",
        "df['age_range'] = pd.cut(df.age, bins, labels = labels,include_lowest = True)\n",
        "\n",
        "df[[x for x in df.columns if '_t' in x] + ['age_range']].groupby('age_range').mean().plot(kind=\"bar\",\n",
        "                                                                              rot=45,\n",
        "                                                                              figsize=(20,10),\n",
        "                                                                              width=0.9,\n",
        "                                                                              color=list('bgc'))\n",
        "_ = plt.xlabel(\"Range of Age\")\n",
        "_ = plt.ylabel(\"Score\")\n",
        "_ = plt.title(\"Total Anxiety, Depression, Stress Scores by Age\")"
      ],
      "metadata": {
        "id": "rok4DoiNkUaJ"
      },
      "id": "rok4DoiNkUaJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[x for x in df.columns if '_p' in x] + ['age_range']].groupby('age_range').mean().plot(kind=\"bar\",\n",
        "                                                                              rot=45,\n",
        "                                                                              figsize=(20,10),\n",
        "                                                                              width=0.9,\n",
        "                                                                              color=list('bgc'))\n",
        "_ = plt.xlabel(\"Range of Age\")\n",
        "_ = plt.ylabel(\"Score\")\n",
        "_ = plt.title(\"Distribution of Physical Symptoms by Age\")"
      ],
      "metadata": {
        "id": "Xh0H50AgkZ-j"
      },
      "id": "Xh0H50AgkZ-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "a4RfVF2TlgTY"
      },
      "id": "a4RfVF2TlgTY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[x for x in df.columns if '_m' in x] + ['age_range']].groupby('age_range').mean().plot(kind=\"bar\",\n",
        "                                                                              rot=45,\n",
        "                                                                              figsize=(20,10),\n",
        "                                                                              width=0.9,\n",
        "                                                                              color=list('bgc'))\n",
        "_ = plt.xlabel(\"Range of Age\")\n",
        "_ = plt.ylabel(\"Score\")\n",
        "_ = plt.title(\"Distribution of Mental Symptoms by Age\")"
      ],
      "metadata": {
        "id": "-Yo2xvx5m56h"
      },
      "id": "-Yo2xvx5m56h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TIPI**"
      ],
      "metadata": {
        "id": "nDF6MGx8nR62"
      },
      "id": "nDF6MGx8nR62"
    },
    {
      "cell_type": "code",
      "source": [
        "tipi_values = df[[i for i in df.columns if 'TIPI' in i]]\n",
        "\n",
        "fig, ax = plt.subplots(2,3, figsize=(12,8))\n",
        "_ = plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "_ = ax[0,0].text(s='TIPI \\nSubscale \\nScores',\n",
        "             x= 0.5, y= 0.5,  \n",
        "             fontsize= 32,\n",
        "             ha='center', va='center', wrap=True,color = 'darkblue')\n",
        "#plots\n",
        "\n",
        "_ = sns.boxplot( y = tipi_values['TIPI_extraversion'], color = 'red',\n",
        "               ax = ax[0,1])\n",
        "\n",
        "_ = sns.boxplot( y = tipi_values['TIPI_agreeableness'], color = 'blue',\n",
        "               ax = ax[0,2])\n",
        "\n",
        "_ = sns.boxplot( y = tipi_values['TIPI_conscientiousness'], color = 'cyan',\n",
        "               ax = ax[1,0])\n",
        "\n",
        "_ = sns.boxplot( y = tipi_values['TIPI_emotional_stability'], color = 'green',\n",
        "               ax = ax[1,1])\n",
        "\n",
        "_ = sns.boxplot( y = tipi_values['TIPI_openness_exp'], color = 'yellow',\n",
        "               ax = ax[1,2])\n",
        "\n",
        "#titles\n",
        "_ = ax[0,1].set_title('Extraversion')\n",
        "_ = ax[0,2].set_title('Agreeableness')\n",
        "_ = ax[1,0].set_title('Conscientiousness')\n",
        "_ = ax[1,1].set_title('Emotional Stability')\n",
        "_ = ax[1,2].set_title('Openness Exp')"
      ],
      "metadata": {
        "id": "D988t1cjnMoy"
      },
      "id": "D988t1cjnMoy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tipi_and_age = df[[x for x in df.columns if 'TIPI' in x] + ['age']]\n",
        "bins = [13, 18, 30, 40, 50, 60, 70, 120]\n",
        "labels = ['13-17','18-29', '30-39', '40-49', '50-59', '60-69', '70+']\n",
        "tipi_and_age['age_range'] = pd.cut(tipi_and_age.age, bins, labels = labels,include_lowest = True)"
      ],
      "metadata": {
        "id": "KXRAW3fcnZ8d"
      },
      "id": "KXRAW3fcnZ8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(3,2, figsize=(18,16))\n",
        "\n",
        "_ = ax[0,0].text(s='TIPI \\nScores by \\nAge',\n",
        "             x= 0.5, y= 0.5,  \n",
        "             fontsize= 32,\n",
        "             ha='center', va='center', wrap=True,color = 'darkblue')\n",
        "\n",
        "_ = sns.barplot(x = 'age_range', y = 'TIPI_extraversion', data = tipi_and_age, ax= ax[0,1])\n",
        "_ = sns.barplot(x = 'age_range', y = 'TIPI_agreeableness', data = tipi_and_age, ax= ax[1,0])\n",
        "_ = sns.barplot(x = 'age_range', y = 'TIPI_conscientiousness', data = tipi_and_age, ax= ax[1,1])\n",
        "_ = sns.barplot(x = 'age_range', y = 'TIPI_emotional_stability', data = tipi_and_age, ax= ax[2,0])\n",
        "_ = sns.barplot(x = 'age_range', y = 'TIPI_openness_exp', data = tipi_and_age, ax= ax[2,1])"
      ],
      "metadata": {
        "id": "lv4sQsOIng5y"
      },
      "id": "lv4sQsOIng5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority of the participants are agreeable and conscientious. Also, each personality type score is increasing by age except extraversion."
      ],
      "metadata": {
        "id": "Z_x_oXuZnoKE"
      },
      "id": "Z_x_oXuZnoKE"
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_plot(df, cmap='RdBu_r'):\n",
        "    fig, ax = plt.subplots(figsize=(12,12))\n",
        "    corr = df.corr()\n",
        "\n",
        "    matshow = ax.matshow(corr, cmap=cmap)\n",
        "    for (x, y), z in np.ndenumerate(corr):\n",
        "        ax.text(y, x, '{:0.2f}'.format(z), ha='center', va='center',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
        "\n",
        "    plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=90)\n",
        "    plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
        "    cb = plt.colorbar(matshow, orientation=\"horizontal\")\n",
        "    ax.tick_params(labelsize=12)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xzvMP6PRns7r"
      },
      "id": "xzvMP6PRns7r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "personality_values = [x for x in df.columns if 'TIPI' in x]\n",
        "personalities = df[personality_values]"
      ],
      "metadata": {
        "id": "22xVeizNoHWX"
      },
      "id": "22xVeizNoHWX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[personality_values]"
      ],
      "metadata": {
        "id": "L3RppbILsskx"
      },
      "id": "L3RppbILsskx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POH-tDRksTkR"
      },
      "id": "POH-tDRksTkR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "MkKQO74atJ1M"
      },
      "id": "MkKQO74atJ1M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dass_headers = ['depression_totalqns','anxiety_totalqns','stress_totalqns']\n",
        "dass_values = df[dass_headers]"
      ],
      "metadata": {
        "id": "qJOEl5VOooP0"
      },
      "id": "qJOEl5VOooP0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character = pd.concat([dass_values, personalities], axis=1)\n",
        "correlation_plot(character, cmap='inferno')"
      ],
      "metadata": {
        "id": "dXWg9FiInwtD"
      },
      "id": "dXWg9FiInwtD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}